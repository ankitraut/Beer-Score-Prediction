{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "from tensorflow.python.saved_model import tag_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data we have\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Data: (185643, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv(\"Beer Train Data Set.csv\", na_values='NaN')\n",
    "df['ABV'] = df['ABV'].fillna(df['ABV'].mean())\n",
    "print('Shape of the Data: ' + str(df.shape))\n",
    "\n",
    "df['Ratings'] = pd.to_numeric(df['Ratings'], errors = 'coerce')\n",
    "df['Ratings'] = df['Ratings'].replace(np.nan, 1.0)\n",
    "ratings_score = df.loc[:,['Ratings', 'Score']]\n",
    "df_use = df[ratings_score['Ratings'] != 0]\n",
    "\n",
    "\n",
    "col_to_encode = ['Style Name']\n",
    "col_to_scale = np.array(['ABV', 'Brewing Company', 'Beer Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling features and target\n",
    "x_raw = df_use.loc[:,col_to_encode+list(col_to_scale)]\n",
    "y = df_use.loc[:,['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and dev sets\n",
    "X_train_raw, X_dev_raw, Y_train, Y_dev = train_test_split(x_raw, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, col_to_encode):\n",
    "    \"\"\"Returns One Hot encoded column\"\"\"\n",
    "    \n",
    "    cat_col = df[col_to_encode]\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(cat_col)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded),1)\n",
    "    \n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    return one_hot_encoded\n",
    "    \n",
    "def scale_col(df, col_to_scale):\n",
    "    \"\"\"Returns a scaled dataframe\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_df = scaler.fit_transform(df[col_to_scale])\n",
    "        \n",
    "        \n",
    "    return scaled_df\n",
    "\n",
    "def process_data(df, col_to_scale, col_to_encode):\n",
    "    \"\"\"Transforms the dataframe into a standard dataframe\n",
    "    with scaled and one hot encoded columns\"\"\"\n",
    "    \n",
    "    index = list(range(0,94)) + list(col_to_scale)\n",
    "    processed_df = pd.DataFrame()\n",
    "    scaled_df = scale_col(df, col_to_scale)\n",
    "    one_hot_encoded = one_hot_encode(df, col_to_encode)\n",
    "    processed_data = np.concatenate([one_hot_encoded, scaled_df], axis = 1) \n",
    "    processed_df = pd.DataFrame(processed_data, columns = index)\n",
    "    \n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (143597, 97)\n",
      "X_dev shape: (15956, 97)\n"
     ]
    }
   ],
   "source": [
    "X_train = process_data(X_train_raw, col_to_scale, col_to_encode)\n",
    "X_dev = process_data(X_dev_raw, col_to_scale, col_to_encode)\n",
    "\n",
    "print('X_train shape: ' + str(X_train.shape))\n",
    "print('X_dev shape: ' + str(X_dev.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the model using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 143597\n",
      "number of dev examples = 15956\n",
      "X_train shape: (97, 143597)\n",
      "Y_train shape: (1, 143597)\n",
      "X_dev shape: (97, 15956)\n",
      "Y_dev shape: (1, 15956)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the trainig and dev data\n",
    "X_train = X_train.T\n",
    "X_dev = X_dev.T\n",
    "Y_train = Y_train.T\n",
    "Y_dev = Y_dev.T\n",
    "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print(\"number of dev examples = \" + str(X_dev.shape[1]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_dev shape: \" + str(X_dev.shape))\n",
    "print(\"Y_dev shape: \" + str(Y_dev.shape))\n",
    "\n",
    "assert(X_train.shape[0] == X_dev.shape[0])\n",
    "assert(X_train.shape[1] == Y_train.shape[1])\n",
    "assert(X_train.shape[0] < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "                       \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    parameters={}\n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable((\"W\"+str(l)), [layer_dims[l], layer_dims[l-1]], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        parameters['b' + str(l)] = tf.get_variable(\"b\"+str(l), [layer_dims[l], 1], initializer = tf.zeros_initializer())\n",
    "\n",
    " \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 200\n",
    "layer_dims = layer_dims=[X_train.shape[0], 25, 12, 6, 1]\n",
    "num_epochs = 2000\n",
    "parameters = {}\n",
    "lambd = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    Z4 = tf.add(tf.matmul(W4, A3), b4)\n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z4, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    m = 143597\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    W4 = parameters[\"W4\"]\n",
    "    \n",
    "    cost = tf.sqrt(tf.losses.mean_squared_error(Z4, Y))\n",
    "    \n",
    "     \n",
    "    #L2_regularization_cost = (lambd/2*m) * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4))\n",
    "    \n",
    "    tot_cost = cost #+ L2_regularization_cost\n",
    "    \n",
    "    return tot_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = X.as_matrix()\n",
    "    Y = Y.as_matrix()\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev, layer_dims=[X_train.shape[0], 25, 12, 6, 1], learning_rate = 0.0001,\n",
    "          num_epochs = 500, minibatch_size = 200, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    parameters = {}\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "   \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z4 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z4, Y, parameters, lambd)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 200 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Dev Accuracy:\", accuracy.eval({X: X_dev, Y: Y_dev}))\n",
    "        \n",
    "        return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\ankit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 3.634329\n",
      "Cost after epoch 200: 0.384099\n",
      "Cost after epoch 400: 0.383419\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 1.0\n",
      "Dev Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W3': array([[-5.40546775e-01, -5.71775973e-01, -5.77599227e-01,\n",
      "        -2.81543463e-01,  1.34015009e-01,  6.76653832e-02,\n",
      "         4.23391789e-01, -1.26005962e-01, -1.09027907e-01,\n",
      "        -1.37332052e-01, -3.10035914e-01, -1.33529246e-01],\n",
      "       [ 5.10601938e-01,  2.55869597e-01, -3.36061716e-02,\n",
      "         1.01884198e+00,  2.25830227e-01,  2.98424035e-01,\n",
      "        -3.50059956e-01, -8.06941807e-01,  8.10044706e-01,\n",
      "        -3.15483779e-01,  9.16739851e-02,  4.11973685e-01],\n",
      "       [-1.65896118e-01, -1.06338926e-01, -4.42245811e-01,\n",
      "        -4.33360189e-02, -3.09423238e-01, -3.67327183e-01,\n",
      "         2.24966466e-01, -2.36748420e-02,  1.17421150e-04,\n",
      "        -2.13919610e-01, -5.04919253e-02,  1.16991848e-02],\n",
      "       [-3.89023662e-01,  3.23603004e-01,  7.27356672e-02,\n",
      "        -7.79896751e-02, -5.20048857e-01, -1.93464637e-01,\n",
      "         3.61025006e-01, -1.77164003e-01, -2.32709587e-01,\n",
      "        -4.58137304e-01, -1.42096236e-01, -5.01641273e-01],\n",
      "       [ 7.34000266e-01, -2.98566699e-01,  2.62086600e-01,\n",
      "        -7.29672074e-01,  2.54896462e-01, -1.07095325e+00,\n",
      "         5.31238735e-01,  3.38256056e-03, -1.32278860e-01,\n",
      "         3.86110067e-01,  4.98195171e-01,  4.11826015e-01],\n",
      "       [-2.66161468e-02,  3.38286519e-01,  7.68691063e-01,\n",
      "        -7.91309059e-01,  2.95963883e-01, -6.44340336e-01,\n",
      "         6.23176396e-01,  8.39129835e-02, -8.40458930e-01,\n",
      "         6.41690269e-02, -1.57620907e-01, -1.90113336e-01]], dtype=float32), 'b4': array([[0.1692173]], dtype=float32), 'W4': array([[-0.7877104 , -1.2756414 , -0.7609471 , -0.32646728,  0.52514577,\n",
      "         0.41122955]], dtype=float32), 'W1': array([[-0.04113305, -0.36754012, -0.30252007, ..., -0.0860184 ,\n",
      "        -0.07576328, -0.0603876 ],\n",
      "       [ 0.06527258,  0.05797803,  0.11451198, ...,  0.1785794 ,\n",
      "        -0.06719983, -0.04383064],\n",
      "       [ 0.17797087,  0.0765968 ,  0.20173109, ..., -0.00744222,\n",
      "         0.11578322,  0.03135025],\n",
      "       ...,\n",
      "       [ 0.33796415,  0.37395537,  0.31460604, ...,  0.17187507,\n",
      "        -0.11018344, -0.21021174],\n",
      "       [ 0.20651324, -0.0257846 ,  0.19071971, ...,  0.20621303,\n",
      "         0.08574009,  0.01218119],\n",
      "       [ 0.04882469, -0.17645365,  0.19643728, ..., -0.00822956,\n",
      "        -0.17004849, -0.00572682]], dtype=float32), 'W2': array([[-1.14572078e-01, -3.30306292e-01, -2.08988354e-01,\n",
      "        -1.09322421e-01,  2.78562516e-01,  3.30773950e-01,\n",
      "         3.70754302e-01,  1.06369339e-01,  1.13988720e-01,\n",
      "         5.96506074e-02, -2.08867431e-01,  8.37716460e-02,\n",
      "         4.48368013e-01,  3.12870175e-01,  1.05013452e-01,\n",
      "         4.79641765e-01,  2.39626318e-01,  7.39509389e-02,\n",
      "        -5.48919082e-01, -6.76944554e-02,  7.14508057e-01,\n",
      "        -4.81284499e-01,  1.78831115e-01,  3.64344686e-01,\n",
      "        -6.16882779e-02],\n",
      "       [-1.08625300e-01, -3.17938536e-01, -6.86467513e-02,\n",
      "        -2.06514165e-01, -3.00872713e-01,  1.60676420e-01,\n",
      "        -4.15163860e-02, -5.45289442e-02, -1.95700109e-01,\n",
      "        -5.72249219e-02, -1.68415830e-02, -2.76942313e-01,\n",
      "         1.76110655e-01,  9.78165343e-02, -9.51590762e-02,\n",
      "        -3.25264752e-01, -1.17214821e-01,  2.88798630e-01,\n",
      "        -1.10725813e-01, -1.71643540e-01, -2.95391500e-01,\n",
      "        -8.41767266e-02, -3.55749786e-01,  3.09826851e-01,\n",
      "        -3.09799373e-01],\n",
      "       [ 2.75023729e-01,  5.03965653e-02,  2.91579187e-01,\n",
      "        -3.31827343e-01,  3.89830500e-01,  1.22884557e-01,\n",
      "         2.93319851e-01,  3.20305854e-01,  3.78273815e-01,\n",
      "         3.51103812e-01, -1.99159071e-01,  4.03125972e-01,\n",
      "         5.61248481e-01, -2.32983068e-01,  1.46910489e-01,\n",
      "         1.35281652e-01,  4.28993285e-01,  5.56149669e-02,\n",
      "        -7.00893700e-01,  7.76074380e-02, -8.90794545e-02,\n",
      "        -4.97754872e-01,  6.28511012e-02,  1.85643896e-01,\n",
      "         1.66470677e-01],\n",
      "       [-9.18552935e-01, -6.75605703e-03,  4.86989692e-02,\n",
      "         1.76891267e-01, -6.36782825e-01,  3.46683562e-01,\n",
      "         4.58445162e-01, -3.52583259e-01,  1.22786626e-01,\n",
      "        -5.14890373e-01,  1.59063682e-01,  1.34044856e-01,\n",
      "        -5.45452833e-02,  3.35728884e-01,  1.18506245e-01,\n",
      "         5.53440750e-01, -2.45721519e-01, -9.72859085e-01,\n",
      "         6.43939152e-02, -4.06686246e-01, -1.38665199e-01,\n",
      "         8.70110095e-02, -3.69275331e-01, -8.23697597e-02,\n",
      "         3.56930941e-01],\n",
      "       [ 1.82547212e-01, -1.92366794e-01, -1.46454275e-01,\n",
      "         1.03273608e-01,  3.47989857e-01,  5.76140225e-01,\n",
      "        -1.35259151e-01,  5.15522540e-01,  2.46540055e-01,\n",
      "        -1.42921403e-01, -3.36870462e-01,  8.14849436e-02,\n",
      "         4.70468134e-01, -2.11648345e-01,  3.37875694e-01,\n",
      "         3.24440122e-01,  2.90035218e-01,  3.91017824e-01,\n",
      "        -2.80180067e-01,  1.18741617e-01,  1.52935043e-01,\n",
      "        -3.39392185e-01,  1.93527043e-01, -1.08549438e-01,\n",
      "         1.41847758e-02],\n",
      "       [-4.79022384e-01, -5.20267367e-01, -6.65090606e-02,\n",
      "        -1.24688947e+00,  1.51217028e-01, -3.73442948e-01,\n",
      "        -1.08634099e-01, -1.73324704e-01,  2.07614705e-01,\n",
      "         2.33564392e-01,  1.89488962e-01, -1.57244429e-01,\n",
      "         4.82546799e-02, -1.57290888e+00, -4.25145239e-01,\n",
      "         4.70014429e-03, -3.20893019e-01,  1.61151215e-01,\n",
      "         2.51366913e-01, -1.97154775e-01,  1.34662300e-01,\n",
      "         2.56582171e-01,  3.59083735e-03, -8.86132643e-02,\n",
      "         3.76863182e-01],\n",
      "       [ 2.78960150e-02,  1.91482186e-01,  2.24902824e-01,\n",
      "        -1.73829272e-01,  5.24956584e-01, -1.49224848e-01,\n",
      "        -1.91264853e-01,  4.71757144e-01,  9.63063985e-02,\n",
      "         1.19965166e-01, -1.34596974e-01,  8.86522904e-02,\n",
      "         5.99871337e-01, -7.95379877e-02,  1.24672383e-01,\n",
      "         2.62616605e-01,  1.19527020e-01,  2.41000831e-01,\n",
      "        -5.93630850e-01,  3.60925168e-01,  1.60590500e-01,\n",
      "        -5.10322273e-01,  4.57448930e-01, -7.60245770e-02,\n",
      "         2.50429511e-01],\n",
      "       [ 1.33515865e-01, -1.22074351e-01,  1.17631182e-02,\n",
      "        -5.40153265e-01, -2.09613457e-01, -3.25843424e-01,\n",
      "         2.48114049e-01, -2.77920902e-01, -1.73616540e-02,\n",
      "         1.86388627e-01, -3.92218918e-01, -4.22038317e-01,\n",
      "         2.87877947e-01,  1.45292863e-01, -1.84220001e-01,\n",
      "        -3.19429159e-01,  2.20805004e-01, -4.31952849e-02,\n",
      "        -2.40399435e-01,  1.56192690e-01,  2.95179933e-01,\n",
      "         2.71591336e-01, -3.65319401e-02,  4.45660472e-01,\n",
      "         6.28787577e-02],\n",
      "       [-6.77806437e-02,  7.59227797e-02, -5.71809590e-01,\n",
      "         3.41017932e-01,  2.24440530e-01, -6.52371645e-02,\n",
      "         1.22221135e-01,  2.32488383e-02, -1.65185958e-01,\n",
      "        -4.33070809e-01, -2.25943580e-01, -1.46764293e-01,\n",
      "         1.80109903e-01,  3.47578861e-02, -1.13291055e-01,\n",
      "         3.70704174e-01, -3.99971843e-01, -1.81736171e-01,\n",
      "         3.44806939e-01, -2.45454520e-01, -8.63194883e-01,\n",
      "         2.77555555e-01,  1.71434498e-04, -6.46910489e-01,\n",
      "         1.27318978e-01],\n",
      "       [ 1.68644354e-01,  3.71528596e-01,  6.26127362e-01,\n",
      "         1.06320858e-01,  3.21887910e-01,  5.08582890e-01,\n",
      "         4.40246969e-01,  4.96008009e-01,  2.80987561e-01,\n",
      "        -5.70257287e-03,  1.81205735e-01, -6.02165423e-02,\n",
      "         2.36218199e-01, -4.21201944e-01,  3.49698693e-01,\n",
      "         5.67748249e-01,  1.98564827e-02,  3.58108222e-01,\n",
      "        -4.69578505e-01, -1.55468762e-01, -2.73101717e-01,\n",
      "        -6.26419783e-01, -1.14567898e-01,  2.26386994e-01,\n",
      "         4.58827754e-03],\n",
      "       [ 5.24169803e-01,  3.07581514e-01, -1.46366611e-01,\n",
      "         3.28944445e-01,  5.59946373e-02,  4.53454733e-01,\n",
      "        -1.57241464e-01, -1.37585670e-01,  4.28410828e-01,\n",
      "         4.88514632e-01, -2.61241436e-01, -1.88807413e-01,\n",
      "        -2.16531843e-01, -8.97363126e-02, -2.51707613e-01,\n",
      "        -7.18102530e-02, -1.19350694e-01,  2.11420543e-02,\n",
      "        -4.76573288e-01,  7.81731606e-02,  4.11391407e-01,\n",
      "        -4.79799882e-02,  2.95262277e-01,  3.92615467e-01,\n",
      "         1.73408866e-01],\n",
      "       [ 1.87660813e-01, -2.78149366e-01,  3.11772615e-01,\n",
      "        -7.07927048e-02,  3.59165251e-01,  1.49321064e-01,\n",
      "         3.83837968e-01,  1.60935044e-01, -5.66530749e-02,\n",
      "         4.27934751e-02,  3.61539930e-01, -8.84580612e-02,\n",
      "         6.86925948e-02,  1.58553764e-01,  2.23591834e-01,\n",
      "         1.51986070e-02, -1.66152522e-01,  4.24973428e-01,\n",
      "        -4.40855443e-01, -1.36036307e-01,  1.07121207e-01,\n",
      "        -2.49806076e-01,  9.06946436e-02,  1.35425597e-01,\n",
      "        -2.78419197e-01]], dtype=float32), 'b3': array([[-0.04484047],\n",
      "       [-0.08720359],\n",
      "       [-0.03865281],\n",
      "       [-0.08664685],\n",
      "       [ 0.21196887],\n",
      "       [ 0.24110103]], dtype=float32), 'b2': array([[ 0.13121566],\n",
      "       [-0.04503151],\n",
      "       [ 0.2504144 ],\n",
      "       [-0.1692142 ],\n",
      "       [ 0.11950966],\n",
      "       [-0.05842722],\n",
      "       [ 0.23763359],\n",
      "       [ 0.12838276],\n",
      "       [ 0.04507701],\n",
      "       [ 0.19872706],\n",
      "       [ 0.21354058],\n",
      "       [ 0.02531731]], dtype=float32), 'b1': array([[ 0.12814444],\n",
      "       [ 0.11437994],\n",
      "       [ 0.25407594],\n",
      "       [-0.05520397],\n",
      "       [ 0.32127723],\n",
      "       [ 0.14634512],\n",
      "       [ 0.1684959 ],\n",
      "       [ 0.27283555],\n",
      "       [ 0.305082  ],\n",
      "       [ 0.21599238],\n",
      "       [-0.08673586],\n",
      "       [ 0.11492422],\n",
      "       [ 0.24901414],\n",
      "       [-0.14122738],\n",
      "       [ 0.19231462],\n",
      "       [ 0.23312551],\n",
      "       [ 0.32704777],\n",
      "       [ 0.28030512],\n",
      "       [-0.18744497],\n",
      "       [ 0.20224205],\n",
      "       [ 0.09587216],\n",
      "       [-0.16370842],\n",
      "       [ 0.2376941 ],\n",
      "       [ 0.30890998],\n",
      "       [ 0.23432419]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Calculated Parameters:\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Test Data: (20628, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABV</th>\n",
       "      <th>Brewing Company</th>\n",
       "      <th>Food Paring</th>\n",
       "      <th>Glassware Used</th>\n",
       "      <th>Beer Name</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Style Name</th>\n",
       "      <th>Cellar Temperature</th>\n",
       "      <th>Serving Temperature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.2</td>\n",
       "      <td>8803</td>\n",
       "      <td>(Curried,Thai)Cheese(pepperyMontereyPepperJack...</td>\n",
       "      <td>PintGlass(orBecker,Nonic,Tumbler),Mug(orSeidel...</td>\n",
       "      <td>34558</td>\n",
       "      <td>3</td>\n",
       "      <td>AmericanIPA</td>\n",
       "      <td>40-45</td>\n",
       "      <td>45-50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3</td>\n",
       "      <td>8558</td>\n",
       "      <td>(Barbecue)Cheese(butteryBrie,Gouda,Havarti,Swi...</td>\n",
       "      <td>PintGlass(orBecker,Nonic,Tumbler),Mug(orSeidel...</td>\n",
       "      <td>86826</td>\n",
       "      <td>20</td>\n",
       "      <td>AmericanPorter</td>\n",
       "      <td>40-45</td>\n",
       "      <td>45-50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7519</td>\n",
       "      <td>Cheese(earthyCamembert,Fontina)General(Aperitif)</td>\n",
       "      <td>Snifter,Tulip,OversizedWineGlass</td>\n",
       "      <td>17051</td>\n",
       "      <td>0</td>\n",
       "      <td>BelgianDarkAle</td>\n",
       "      <td>45-50</td>\n",
       "      <td>45-50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.2</td>\n",
       "      <td>9852</td>\n",
       "      <td>(LatinAmerican,German)Meat(Pork,Poultry)</td>\n",
       "      <td>Flute,PilsenerGlass(orPokal),Mug(orSeidel,Stein)</td>\n",
       "      <td>49156</td>\n",
       "      <td>2</td>\n",
       "      <td>ViennaLager</td>\n",
       "      <td>35-40</td>\n",
       "      <td>40-45</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.1</td>\n",
       "      <td>8991</td>\n",
       "      <td>(Barbecue)Cheese(butteryBrie,Gouda,Havarti,Swi...</td>\n",
       "      <td>PintGlass(orBecker,Nonic,Tumbler),Mug(orSeidel...</td>\n",
       "      <td>162723</td>\n",
       "      <td>5</td>\n",
       "      <td>AmericanPorter</td>\n",
       "      <td>40-45</td>\n",
       "      <td>45-50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ABV  Brewing Company                                        Food Paring  \\\n",
       "0  6.2             8803  (Curried,Thai)Cheese(pepperyMontereyPepperJack...   \n",
       "1  5.3             8558  (Barbecue)Cheese(butteryBrie,Gouda,Havarti,Swi...   \n",
       "2  7.0             7519   Cheese(earthyCamembert,Fontina)General(Aperitif)   \n",
       "3  5.2             9852           (LatinAmerican,German)Meat(Pork,Poultry)   \n",
       "4  8.1             8991  (Barbecue)Cheese(butteryBrie,Gouda,Havarti,Swi...   \n",
       "\n",
       "                                      Glassware Used  Beer Name Ratings  \\\n",
       "0  PintGlass(orBecker,Nonic,Tumbler),Mug(orSeidel...      34558       3   \n",
       "1  PintGlass(orBecker,Nonic,Tumbler),Mug(orSeidel...      86826      20   \n",
       "2                   Snifter,Tulip,OversizedWineGlass      17051       0   \n",
       "3   Flute,PilsenerGlass(orPokal),Mug(orSeidel,Stein)      49156       2   \n",
       "4  PintGlass(orBecker,Nonic,Tumbler),Mug(orSeidel...     162723       5   \n",
       "\n",
       "       Style Name Cellar Temperature Serving Temperature  Score  \n",
       "0     AmericanIPA              40-45               45-50    0.0  \n",
       "1  AmericanPorter              40-45               45-50    0.0  \n",
       "2  BelgianDarkAle              45-50               45-50    0.0  \n",
       "3     ViennaLager              35-40               40-45    0.0  \n",
       "4  AmericanPorter              40-45               45-50    0.0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the test data:\n",
    "test_raw = pd.read_csv('Beer Test Data Set.csv', na_values=np.nan)\n",
    "test_raw = test_raw.fillna(0)\n",
    "print('Shape of the Test Data: ' + str(test_raw.shape))\n",
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw['Ratings'] = pd.to_numeric(test_raw['Ratings'], errors = 'coerce')\n",
    "test_raw['Ratings'] = test_raw['Ratings'].replace(np.nan, 1.0)\n",
    "ratings = test_raw.loc[:,['Ratings']]\n",
    "test_nonzero = test_raw[ratings['Ratings'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "test_use = test_nonzero.loc[:,col_to_encode+list(col_to_scale)]\n",
    "test_norm = process_data(test_use, col_to_scale, col_to_encode) \n",
    "test_norm = test_norm.astype('float32').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 17642)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = forward_propagation(test_norm, parameters)\n",
    "with tf.Session() as sess:\n",
    "    values = temp.eval()\n",
    "values_list = values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17642"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_generator(values_list):\n",
    "    for value in values_list:\n",
    "        yield value\n",
    "        \n",
    "value = value_generator(values_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = []\n",
    "try:\n",
    "    for i, status in enumerate(ratings['Ratings']==0):\n",
    "        if ratings.iloc[i, :].item() == 0:\n",
    "            predicted_values.append(0.0)\n",
    "        else:\n",
    "            predicted_values.append(next(value))\n",
    "except StopIteration:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.866166830062866,\n",
       " 3.7683444023132324,\n",
       " 3.540391206741333,\n",
       " 3.8409583568573,\n",
       " 4.009626388549805,\n",
       " 4.0543317794799805,\n",
       " 2.9862139225006104,\n",
       " 3.9763293266296387,\n",
       " 3.789870023727417,\n",
       " 3.7154409885406494,\n",
       " 3.815401315689087,\n",
       " 3.9896013736724854,\n",
       " 3.5048015117645264,\n",
       " 3.5362794399261475,\n",
       " 3.4976389408111572,\n",
       " 3.897258996963501,\n",
       " 3.896641492843628,\n",
       " 3.71805739402771,\n",
       " 3.0221126079559326,\n",
       " 3.8510100841522217,\n",
       " 3.2057695388793945,\n",
       " 3.5752127170562744,\n",
       " 3.8971145153045654,\n",
       " 3.6277174949645996,\n",
       " 4.066475868225098,\n",
       " 3.5949461460113525,\n",
       " 3.661794900894165,\n",
       " 3.870958089828491,\n",
       " 3.7018470764160156,\n",
       " 3.7025372982025146,\n",
       " 3.775808811187744,\n",
       " 3.8267438411712646,\n",
       " 3.553178548812866,\n",
       " 3.821864366531372,\n",
       " 3.9019672870635986,\n",
       " 3.7437942028045654,\n",
       " 3.736920118331909,\n",
       " 3.803417444229126,\n",
       " 3.8591089248657227,\n",
       " 3.716827154159546,\n",
       " 3.056966543197632,\n",
       " 3.8754734992980957,\n",
       " 2.7215960025787354,\n",
       " 3.689203977584839,\n",
       " 3.989039659500122,\n",
       " 3.7306768894195557,\n",
       " 3.590235471725464,\n",
       " 3.748295783996582,\n",
       " 3.649200677871704,\n",
       " 3.813464879989624,\n",
       " 3.6759941577911377,\n",
       " 4.092890739440918,\n",
       " 3.638104200363159,\n",
       " 3.6395976543426514,\n",
       " 3.730064868927002,\n",
       " 3.788200616836548,\n",
       " 4.057406902313232,\n",
       " 3.7800848484039307,\n",
       " 3.693155288696289,\n",
       " 3.7495739459991455,\n",
       " 3.997070074081421,\n",
       " 3.7215540409088135,\n",
       " 3.3803834915161133,\n",
       " 3.1544413566589355,\n",
       " 3.929635763168335,\n",
       " 3.905273199081421,\n",
       " 3.817551851272583,\n",
       " 3.9974257946014404,\n",
       " 3.708146095275879,\n",
       " 3.8162996768951416,\n",
       " 3.8789024353027344,\n",
       " 3.765939474105835,\n",
       " 3.5969936847686768,\n",
       " 3.7638866901397705,\n",
       " 3.7099459171295166,\n",
       " 3.567387819290161,\n",
       " 4.017168045043945,\n",
       " 3.5388343334198,\n",
       " 3.767305850982666,\n",
       " 3.590191125869751,\n",
       " 2.263209581375122,\n",
       " 3.8044207096099854,\n",
       " 4.015867710113525,\n",
       " 3.9256937503814697,\n",
       " 3.643089532852173,\n",
       " 3.8566274642944336,\n",
       " 3.7035675048828125,\n",
       " 3.801255941390991,\n",
       " 3.862335205078125,\n",
       " 3.8651764392852783,\n",
       " 3.829249143600464,\n",
       " 4.020252227783203,\n",
       " 3.570582628250122,\n",
       " 3.853749990463257,\n",
       " 3.9962613582611084,\n",
       " 3.4940712451934814,\n",
       " 3.919602632522583,\n",
       " 3.8118159770965576,\n",
       " 3.9587345123291016,\n",
       " 4.043596267700195,\n",
       " 4.023397445678711,\n",
       " 3.635096311569214,\n",
       " 3.7773053646087646,\n",
       " 3.8666200637817383,\n",
       " 3.548192262649536,\n",
       " 3.875244617462158,\n",
       " 3.6817781925201416,\n",
       " 3.8560173511505127,\n",
       " 3.910513401031494,\n",
       " 3.9825689792633057,\n",
       " 3.6300761699676514,\n",
       " 3.6852786540985107,\n",
       " 3.83441424369812,\n",
       " 3.6433675289154053,\n",
       " 3.655535936355591,\n",
       " 3.877256155014038,\n",
       " 3.5439538955688477,\n",
       " 3.5895912647247314,\n",
       " 3.9609763622283936,\n",
       " 3.720500946044922,\n",
       " 3.79424786567688,\n",
       " 3.872450828552246,\n",
       " 3.7686567306518555,\n",
       " 3.8029887676239014,\n",
       " 3.013965368270874,\n",
       " 2.8223421573638916,\n",
       " 3.2739832401275635,\n",
       " 3.979250907897949,\n",
       " 3.8103463649749756,\n",
       " 3.807237386703491,\n",
       " 3.804861307144165,\n",
       " 3.9968535900115967,\n",
       " 3.890612840652466,\n",
       " 3.8476173877716064,\n",
       " 3.621668577194214,\n",
       " 3.508296012878418,\n",
       " 3.5606281757354736,\n",
       " 3.3542420864105225,\n",
       " 3.74926495552063,\n",
       " 3.6348588466644287,\n",
       " 3.5633742809295654,\n",
       " 3.8220179080963135,\n",
       " 3.735398530960083,\n",
       " 3.758570432662964,\n",
       " 3.7288100719451904,\n",
       " 3.567304849624634,\n",
       " 3.9817864894866943,\n",
       " 3.5138378143310547,\n",
       " 3.902231216430664,\n",
       " 3.6577208042144775,\n",
       " 3.6073367595672607,\n",
       " 3.708883285522461,\n",
       " 3.8651626110076904,\n",
       " 3.7676689624786377,\n",
       " 3.6431643962860107,\n",
       " 3.7645633220672607,\n",
       " 3.2997689247131348,\n",
       " 3.6275546550750732,\n",
       " 3.9478909969329834,\n",
       " 3.5885069370269775,\n",
       " 3.458566904067993,\n",
       " 3.5497536659240723,\n",
       " 3.812350034713745,\n",
       " 3.5883381366729736,\n",
       " 3.7265748977661133,\n",
       " 3.857684373855591,\n",
       " 2.8064472675323486,\n",
       " 3.9798171520233154,\n",
       " 3.5064291954040527,\n",
       " 3.4467039108276367,\n",
       " 3.8362200260162354,\n",
       " 3.7894749641418457,\n",
       " 3.7519419193267822,\n",
       " 3.839935064315796,\n",
       " 3.765856981277466,\n",
       " 3.5836098194122314,\n",
       " 3.5106589794158936,\n",
       " 3.8066320419311523,\n",
       " 3.5853450298309326,\n",
       " 3.81447434425354,\n",
       " 3.8699419498443604,\n",
       " 3.557921886444092,\n",
       " 3.7344586849212646,\n",
       " 3.806262254714966,\n",
       " 4.066612720489502,\n",
       " 3.6921443939208984,\n",
       " 3.8021962642669678,\n",
       " 3.813339948654175,\n",
       " 3.5952343940734863,\n",
       " 3.6108896732330322,\n",
       " 3.6311020851135254,\n",
       " 3.4442243576049805,\n",
       " 3.8650763034820557,\n",
       " 3.857530117034912,\n",
       " 2.9805448055267334,\n",
       " 3.7474429607391357,\n",
       " 2.781229257583618,\n",
       " 3.9196178913116455,\n",
       " 3.9039511680603027,\n",
       " 4.006499290466309,\n",
       " 3.970950126647949,\n",
       " 3.7363827228546143,\n",
       " 3.837392568588257,\n",
       " 3.8938186168670654,\n",
       " 3.0630252361297607,\n",
       " 3.4641213417053223,\n",
       " 3.752061128616333,\n",
       " 3.8054797649383545,\n",
       " 3.9135804176330566,\n",
       " 3.7782042026519775,\n",
       " 3.6299211978912354,\n",
       " 3.6584181785583496,\n",
       " 3.8171544075012207,\n",
       " 3.652754306793213,\n",
       " 3.7830698490142822,\n",
       " 3.5674312114715576,\n",
       " 3.3747198581695557,\n",
       " 3.4972822666168213,\n",
       " 2.7800683975219727,\n",
       " 4.137490749359131,\n",
       " 3.4628493785858154,\n",
       " 3.7827794551849365,\n",
       " 3.8349924087524414,\n",
       " 3.610314130783081,\n",
       " 3.5772485733032227,\n",
       " 3.7293403148651123,\n",
       " 3.6943843364715576,\n",
       " 3.428306818008423,\n",
       " 3.874457359313965,\n",
       " 3.8248817920684814,\n",
       " 3.308044195175171,\n",
       " 3.6276729106903076,\n",
       " 3.6993892192840576,\n",
       " 3.887624502182007,\n",
       " 3.86702561378479,\n",
       " 3.6313140392303467,\n",
       " 3.891874074935913,\n",
       " 3.7271387577056885,\n",
       " 3.860867738723755,\n",
       " 3.8617358207702637,\n",
       " 3.7279069423675537,\n",
       " 3.733508348464966,\n",
       " 3.9852616786956787,\n",
       " 4.052407741546631,\n",
       " 3.854754686355591,\n",
       " 3.8567001819610596,\n",
       " 3.924638032913208,\n",
       " 3.4651384353637695,\n",
       " 3.912571907043457,\n",
       " 3.7292659282684326,\n",
       " 3.8654210567474365,\n",
       " 2.9027445316314697,\n",
       " 3.9307353496551514,\n",
       " 3.5780017375946045,\n",
       " 3.9858736991882324,\n",
       " 3.602257013320923,\n",
       " 3.9402077198028564,\n",
       " 3.689502000808716,\n",
       " 3.8360302448272705,\n",
       " 3.0289430618286133,\n",
       " 3.670440435409546,\n",
       " 3.5848615169525146,\n",
       " 4.005716323852539,\n",
       " 3.3023569583892822,\n",
       " 3.006878137588501,\n",
       " 2.756436586380005,\n",
       " 3.8590023517608643,\n",
       " 3.8361966609954834,\n",
       " 3.5812230110168457,\n",
       " 3.8958075046539307,\n",
       " 3.6358916759490967,\n",
       " 3.722527265548706,\n",
       " 2.915640354156494,\n",
       " 3.459066152572632,\n",
       " 3.173027276992798,\n",
       " 3.7460591793060303,\n",
       " 3.578735589981079,\n",
       " 3.9269042015075684,\n",
       " 3.8747575283050537,\n",
       " 3.5570461750030518,\n",
       " 3.806584119796753,\n",
       " 3.748227834701538,\n",
       " 3.5888235569000244,\n",
       " 3.879394054412842,\n",
       " 3.4649646282196045,\n",
       " 3.8645660877227783,\n",
       " 3.5518250465393066,\n",
       " 3.9246160984039307,\n",
       " 3.5659658908843994,\n",
       " 3.393305540084839,\n",
       " 3.5279242992401123,\n",
       " 4.036511421203613,\n",
       " 3.800333261489868,\n",
       " 3.904421091079712,\n",
       " 3.6110734939575195,\n",
       " 3.8375484943389893,\n",
       " 3.7320263385772705,\n",
       " 3.9490044116973877,\n",
       " 3.86811900138855,\n",
       " 3.6030337810516357,\n",
       " 3.1925556659698486,\n",
       " 3.5936028957366943,\n",
       " 3.5683274269104004,\n",
       " 3.7890923023223877,\n",
       " 3.76344895362854,\n",
       " 3.535338878631592,\n",
       " 3.825550079345703,\n",
       " 3.903162717819214,\n",
       " 2.9103376865386963,\n",
       " 4.002922534942627,\n",
       " 4.009580135345459,\n",
       " 3.425452709197998,\n",
       " 3.722466230392456,\n",
       " 3.7657792568206787,\n",
       " 3.703681707382202,\n",
       " 3.9079723358154297,\n",
       " 3.7980997562408447,\n",
       " 3.851644277572632,\n",
       " 3.725264310836792,\n",
       " 3.7283365726470947,\n",
       " 3.6239821910858154,\n",
       " 3.6215436458587646,\n",
       " 3.739884376525879,\n",
       " 3.968003988265991,\n",
       " 3.9013543128967285,\n",
       " 3.7770495414733887,\n",
       " 3.4728991985321045,\n",
       " 3.7862744331359863,\n",
       " 3.989715814590454,\n",
       " 3.676408529281616,\n",
       " 2.9692249298095703,\n",
       " 3.7902510166168213,\n",
       " 3.883575677871704,\n",
       " 3.863067388534546,\n",
       " 4.051860809326172,\n",
       " 3.622058153152466,\n",
       " 3.5410993099212646,\n",
       " 3.8907835483551025,\n",
       " 3.504020929336548,\n",
       " 4.043876647949219,\n",
       " 3.724440336227417,\n",
       " 3.7189552783966064,\n",
       " 3.9918839931488037,\n",
       " 4.0294036865234375,\n",
       " 3.6770942211151123,\n",
       " 3.4886367321014404,\n",
       " 3.970496416091919,\n",
       " 4.003502368927002,\n",
       " 3.865354061126709,\n",
       " 3.785759210586548,\n",
       " 4.013347148895264,\n",
       " 3.7030112743377686,\n",
       " 3.5375514030456543,\n",
       " 3.343022108078003,\n",
       " 3.7414040565490723,\n",
       " 3.669642686843872,\n",
       " 3.951833486557007,\n",
       " 3.844741106033325,\n",
       " 3.7791006565093994,\n",
       " 3.648453950881958,\n",
       " 3.4648029804229736,\n",
       " 3.876812696456909,\n",
       " 3.5940048694610596,\n",
       " 3.5809249877929688,\n",
       " 2.9609768390655518,\n",
       " 3.4481823444366455,\n",
       " 3.930748224258423,\n",
       " 3.868162155151367,\n",
       " 4.0313615798950195,\n",
       " 3.3897199630737305,\n",
       " 3.6707088947296143,\n",
       " 3.821549654006958,\n",
       " 3.7425291538238525,\n",
       " 3.5665714740753174,\n",
       " 3.775707483291626,\n",
       " 4.058980941772461,\n",
       " 4.123647689819336,\n",
       " 3.6267306804656982,\n",
       " 3.8933675289154053,\n",
       " 3.7869248390197754,\n",
       " 3.9136531352996826,\n",
       " 3.9265544414520264,\n",
       " 3.6667990684509277,\n",
       " 3.8502681255340576,\n",
       " 3.890202283859253,\n",
       " 3.2881875038146973,\n",
       " 3.8833506107330322,\n",
       " 3.4528164863586426,\n",
       " 3.7060554027557373,\n",
       " 3.673591375350952,\n",
       " 3.5611584186553955,\n",
       " 3.853505849838257,\n",
       " 3.9889566898345947,\n",
       " 3.824392795562744,\n",
       " 3.5795586109161377,\n",
       " 3.7814862728118896,\n",
       " 3.759786367416382,\n",
       " 3.7286434173583984,\n",
       " 3.547180414199829,\n",
       " 3.7654647827148438,\n",
       " 3.8366997241973877,\n",
       " 2.7751457691192627,\n",
       " 3.5546658039093018,\n",
       " 3.713115930557251,\n",
       " 3.4120514392852783,\n",
       " 3.564302921295166,\n",
       " 3.9015867710113525,\n",
       " 4.000147819519043,\n",
       " 3.5185086727142334,\n",
       " 3.506620168685913,\n",
       " 3.7012226581573486,\n",
       " 3.4031224250793457,\n",
       " 3.4873228073120117,\n",
       " 3.6442205905914307,\n",
       " 3.748842477798462,\n",
       " 3.9530069828033447,\n",
       " 4.049716949462891,\n",
       " 4.018846035003662,\n",
       " 3.515679121017456,\n",
       " 3.7700107097625732,\n",
       " 4.018180847167969,\n",
       " 4.0152268409729,\n",
       " 3.9768145084381104,\n",
       " 3.906508684158325,\n",
       " 3.9325873851776123,\n",
       " 3.743582010269165,\n",
       " 3.5424420833587646,\n",
       " 4.037196636199951,\n",
       " 3.827238082885742,\n",
       " 3.6299965381622314,\n",
       " 3.7037394046783447,\n",
       " 3.861030340194702,\n",
       " 3.0394089221954346,\n",
       " 3.8679468631744385,\n",
       " 3.8762428760528564,\n",
       " 3.790269136428833,\n",
       " 3.6936662197113037,\n",
       " 3.7998263835906982,\n",
       " 3.762307643890381,\n",
       " 3.6037139892578125,\n",
       " 3.7072980403900146,\n",
       " 3.2687911987304688,\n",
       " 3.4733407497406006,\n",
       " 3.8258330821990967,\n",
       " 3.7873799800872803,\n",
       " 3.766441822052002,\n",
       " 3.5987355709075928,\n",
       " 3.6488125324249268,\n",
       " 3.62732195854187,\n",
       " 3.9152755737304688,\n",
       " 3.698899030685425,\n",
       " 3.971384286880493,\n",
       " 3.5572850704193115,\n",
       " 3.8050291538238525,\n",
       " 3.9096553325653076,\n",
       " 4.017490863800049,\n",
       " 3.3051559925079346,\n",
       " 3.789437770843506,\n",
       " 3.6666080951690674,\n",
       " 2.911860227584839,\n",
       " 3.820831060409546,\n",
       " 3.924700975418091,\n",
       " 3.6821162700653076,\n",
       " 3.671107530593872,\n",
       " 3.6280951499938965,\n",
       " 3.695033311843872,\n",
       " 3.2787601947784424,\n",
       " 4.03358793258667,\n",
       " 3.1330997943878174,\n",
       " 3.9110817909240723,\n",
       " 3.0313823223114014,\n",
       " 3.7610599994659424,\n",
       " 3.9460833072662354,\n",
       " 3.4991514682769775,\n",
       " 4.049507141113281,\n",
       " 3.604779005050659,\n",
       " 3.881561040878296,\n",
       " 3.628483533859253,\n",
       " 3.9051432609558105,\n",
       " 3.613997459411621,\n",
       " 3.910723924636841,\n",
       " 3.849527597427368,\n",
       " 3.762190580368042,\n",
       " 3.6572768688201904,\n",
       " 3.609819173812866,\n",
       " 3.893275499343872,\n",
       " 3.7114245891571045,\n",
       " 3.7056455612182617,\n",
       " 3.731950283050537,\n",
       " 3.557879686355591,\n",
       " 3.5839831829071045,\n",
       " 3.7885243892669678,\n",
       " 3.7483177185058594,\n",
       " 3.7983994483947754,\n",
       " 3.691206932067871,\n",
       " 3.898946762084961,\n",
       " 3.6031711101531982,\n",
       " 3.994342088699341,\n",
       " 4.093465328216553,\n",
       " 3.872460126876831,\n",
       " 3.452376127243042,\n",
       " 3.579578161239624,\n",
       " 3.683072328567505,\n",
       " 3.901679277420044,\n",
       " 3.918081045150757,\n",
       " 4.0231428146362305,\n",
       " 3.703890323638916,\n",
       " 3.9866645336151123,\n",
       " 3.893110513687134,\n",
       " 3.679180383682251,\n",
       " 3.875839948654175,\n",
       " 4.0110578536987305,\n",
       " 3.2789876461029053,\n",
       " 3.825263261795044,\n",
       " 2.823451042175293,\n",
       " 3.838361978530884,\n",
       " 3.5724294185638428,\n",
       " 3.551983594894409,\n",
       " 4.030739784240723,\n",
       " 3.9635159969329834,\n",
       " 3.873492479324341,\n",
       " 3.50357985496521,\n",
       " 3.7267441749572754,\n",
       " 4.027934551239014,\n",
       " 3.5329642295837402,\n",
       " 4.01434326171875,\n",
       " 3.5527756214141846,\n",
       " 3.827216625213623,\n",
       " 3.7516629695892334,\n",
       " 3.767918109893799,\n",
       " 3.4154245853424072,\n",
       " 3.894885301589966,\n",
       " 2.8099424839019775,\n",
       " 3.529304027557373,\n",
       " 3.7518155574798584,\n",
       " 3.8606717586517334,\n",
       " 3.8823869228363037,\n",
       " 3.9950380325317383,\n",
       " 3.84565806388855,\n",
       " 3.7342398166656494,\n",
       " 3.6747798919677734,\n",
       " 3.8728134632110596,\n",
       " 3.3323426246643066,\n",
       " 3.770174741744995,\n",
       " 3.5741708278656006,\n",
       " 3.715320587158203,\n",
       " 3.833055257797241,\n",
       " 3.624310255050659,\n",
       " 3.860563039779663,\n",
       " 3.9270503520965576,\n",
       " 3.693660020828247,\n",
       " 3.57886004447937,\n",
       " 3.7974796295166016,\n",
       " 3.699188470840454,\n",
       " 3.783083200454712,\n",
       " 3.606804609298706,\n",
       " 3.78743839263916,\n",
       " 3.732994794845581,\n",
       " 4.122922897338867,\n",
       " 3.791517972946167,\n",
       " 3.5226824283599854,\n",
       " 3.977299451828003,\n",
       " 3.1856577396392822,\n",
       " 3.9443018436431885,\n",
       " 3.623286485671997,\n",
       " 3.5924108028411865,\n",
       " 3.9136385917663574,\n",
       " 3.870252847671509,\n",
       " 3.813110828399658,\n",
       " 3.547480821609497,\n",
       " 3.7660393714904785,\n",
       " 3.7808473110198975,\n",
       " 3.887678623199463,\n",
       " 3.8874518871307373,\n",
       " 3.995333433151245,\n",
       " 3.7699639797210693,\n",
       " 3.796111822128296,\n",
       " 3.657500982284546,\n",
       " 3.6754424571990967,\n",
       " 3.7801637649536133,\n",
       " 3.774632215499878,\n",
       " 3.710458993911743,\n",
       " 3.4849510192871094,\n",
       " 3.765075922012329,\n",
       " 3.6741645336151123,\n",
       " 3.5924296379089355,\n",
       " 3.8738934993743896,\n",
       " 3.6043484210968018,\n",
       " 3.8050501346588135,\n",
       " 3.917917013168335,\n",
       " 3.5819222927093506,\n",
       " 3.9711577892303467,\n",
       " 3.8054921627044678,\n",
       " 3.583867311477661,\n",
       " 3.9195773601531982,\n",
       " 3.7463231086730957,\n",
       " 3.6649558544158936,\n",
       " 3.7236201763153076,\n",
       " 4.1277947425842285,\n",
       " 3.983877182006836,\n",
       " 3.8123056888580322,\n",
       " 3.857760429382324,\n",
       " 3.026472330093384,\n",
       " 3.813549280166626,\n",
       " 3.8269364833831787,\n",
       " 3.8874924182891846,\n",
       " 3.6625235080718994,\n",
       " 3.528721570968628,\n",
       " 3.760075807571411,\n",
       " 3.835145950317383,\n",
       " 3.809821367263794,\n",
       " 3.8324077129364014,\n",
       " 3.6042277812957764,\n",
       " 3.680964708328247,\n",
       " 3.8559553623199463,\n",
       " 3.796066999435425,\n",
       " 3.85410737991333,\n",
       " 3.620044469833374,\n",
       " 3.8861606121063232,\n",
       " 4.0080790519714355,\n",
       " 3.5315849781036377,\n",
       " 3.9039316177368164,\n",
       " 2.9986326694488525,\n",
       " 3.6237103939056396,\n",
       " 3.4932334423065186,\n",
       " 3.5540568828582764,\n",
       " 4.017675876617432,\n",
       " 3.6421074867248535,\n",
       " 3.8343899250030518,\n",
       " 2.4443204402923584,\n",
       " 3.4582602977752686,\n",
       " 3.694967031478882,\n",
       " 3.8853888511657715,\n",
       " 3.7147481441497803,\n",
       " 3.5570929050445557,\n",
       " 3.665412664413452,\n",
       " 3.8305823802948,\n",
       " 3.5578114986419678,\n",
       " 3.63107967376709,\n",
       " 3.921459436416626,\n",
       " 3.685800790786743,\n",
       " 3.605041742324829,\n",
       " 3.815082550048828,\n",
       " 3.450268268585205,\n",
       " 3.5322375297546387,\n",
       " 3.8577873706817627,\n",
       " 3.5928921699523926,\n",
       " 3.5834200382232666,\n",
       " 3.8226592540740967,\n",
       " 3.9861435890197754,\n",
       " 3.7169253826141357,\n",
       " 3.7578394412994385,\n",
       " 3.904637098312378,\n",
       " 3.907064437866211,\n",
       " 3.7806546688079834,\n",
       " 3.631528377532959,\n",
       " 3.733436107635498,\n",
       " 3.5719192028045654,\n",
       " 3.8583455085754395,\n",
       " 3.6272683143615723,\n",
       " 3.8124008178710938,\n",
       " 3.8744595050811768,\n",
       " 3.9267256259918213,\n",
       " 3.5871002674102783,\n",
       " 3.904979705810547,\n",
       " 3.830657482147217,\n",
       " 3.5283427238464355,\n",
       " 3.7725276947021484,\n",
       " 3.7885921001434326,\n",
       " 3.702115774154663,\n",
       " 3.611524820327759,\n",
       " 3.5624022483825684,\n",
       " 3.533870220184326,\n",
       " 3.5402348041534424,\n",
       " 3.7525041103363037,\n",
       " 3.917600631713867,\n",
       " 3.770171880722046,\n",
       " 3.615685224533081,\n",
       " 3.8416359424591064,\n",
       " 3.7090423107147217,\n",
       " 3.8307864665985107,\n",
       " 3.9544386863708496,\n",
       " 3.721000909805298,\n",
       " 3.58660888671875,\n",
       " 3.7433197498321533,\n",
       " 3.610454797744751,\n",
       " 3.6194937229156494,\n",
       " 3.757528066635132,\n",
       " 3.6661527156829834,\n",
       " 3.54085373878479,\n",
       " 3.8630621433258057,\n",
       " 3.64780855178833,\n",
       " 3.593904733657837,\n",
       " 3.688682794570923,\n",
       " 3.9114084243774414,\n",
       " 3.8563039302825928,\n",
       " 3.738237142562866,\n",
       " 3.5787241458892822,\n",
       " 3.0951168537139893,\n",
       " 3.860787868499756,\n",
       " 2.9237992763519287,\n",
       " 3.850653886795044,\n",
       " 3.943223237991333,\n",
       " 3.38362193107605,\n",
       " 3.6133475303649902,\n",
       " 3.9017107486724854,\n",
       " 3.840827465057373,\n",
       " 4.078001022338867,\n",
       " 3.9862260818481445,\n",
       " 3.764342784881592,\n",
       " 3.934316635131836,\n",
       " 3.414686441421509,\n",
       " 3.7510201930999756,\n",
       " 3.673360824584961,\n",
       " 3.5638577938079834,\n",
       " 3.9112164974212646,\n",
       " 3.885061502456665,\n",
       " 3.407792568206787,\n",
       " 4.130478858947754,\n",
       " 3.8570683002471924,\n",
       " 3.6854331493377686,\n",
       " 3.74568247795105,\n",
       " 3.7066080570220947,\n",
       " 3.8452699184417725,\n",
       " 3.42638897895813,\n",
       " 3.865737199783325,\n",
       " 3.920944929122925,\n",
       " 3.7910311222076416,\n",
       " 3.4884049892425537,\n",
       " 3.760434865951538,\n",
       " 3.833031415939331,\n",
       " 3.8143270015716553,\n",
       " 3.917001962661743,\n",
       " 3.722540855407715,\n",
       " 3.5915443897247314,\n",
       " 3.813951253890991,\n",
       " 3.802541971206665,\n",
       " 3.8798182010650635,\n",
       " 3.8452951908111572,\n",
       " 3.9668943881988525,\n",
       " 4.09271764755249,\n",
       " 3.817134141921997,\n",
       " 3.9195806980133057,\n",
       " 3.7682058811187744,\n",
       " 3.4349334239959717,\n",
       " 3.8686604499816895,\n",
       " 3.6232962608337402,\n",
       " 2.377913236618042,\n",
       " 3.66626238822937,\n",
       " 3.7497994899749756,\n",
       " 3.853973150253296,\n",
       " 3.868478536605835,\n",
       " 3.842618465423584,\n",
       " 4.043659687042236,\n",
       " 3.8330769538879395,\n",
       " 3.9167239665985107,\n",
       " 3.7539048194885254,\n",
       " 3.81364369392395,\n",
       " 3.5092356204986572,\n",
       " 3.6418802738189697,\n",
       " 3.8873746395111084,\n",
       " 3.8378429412841797,\n",
       " 4.007636547088623,\n",
       " 3.485607385635376,\n",
       " 3.3902065753936768,\n",
       " 3.7162024974823,\n",
       " 3.8169591426849365,\n",
       " 3.737055778503418,\n",
       " 3.8929359912872314,\n",
       " 3.817007541656494,\n",
       " 3.3629486560821533,\n",
       " 4.018795490264893,\n",
       " 3.833420753479004,\n",
       " 3.912358522415161,\n",
       " 2.973468065261841,\n",
       " 3.8267085552215576,\n",
       " 3.4797139167785645,\n",
       " 3.8485844135284424,\n",
       " 3.852386474609375,\n",
       " 3.3777999877929688,\n",
       " 3.6803205013275146,\n",
       " 3.5919406414031982,\n",
       " 3.30501389503479,\n",
       " 3.8619511127471924,\n",
       " 3.5350329875946045,\n",
       " 2.889103651046753,\n",
       " 3.689443349838257,\n",
       " 3.794750928878784,\n",
       " 3.604095697402954,\n",
       " 3.767047166824341,\n",
       " 4.082469463348389,\n",
       " 3.149611711502075,\n",
       " 3.642556667327881,\n",
       " 3.954265832901001,\n",
       " 3.584596872329712,\n",
       " 3.435096025466919,\n",
       " 3.7047760486602783,\n",
       " 3.8442928791046143,\n",
       " 3.7690722942352295,\n",
       " 3.8531100749969482,\n",
       " 3.6356613636016846,\n",
       " 3.7174384593963623,\n",
       " 3.2945196628570557,\n",
       " 3.6413767337799072,\n",
       " 3.7182300090789795,\n",
       " 3.5244929790496826,\n",
       " 3.8216733932495117,\n",
       " 3.701491117477417,\n",
       " 3.9846973419189453,\n",
       " 3.7678918838500977,\n",
       " 3.5711631774902344,\n",
       " 3.87911057472229,\n",
       " 3.8529212474823,\n",
       " 3.810621500015259,\n",
       " 3.6257522106170654,\n",
       " 3.7926905155181885,\n",
       " 3.710096597671509,\n",
       " 3.5777626037597656,\n",
       " 3.7954952716827393,\n",
       " 4.02822732925415,\n",
       " 3.7480247020721436,\n",
       " 3.885331392288208,\n",
       " 3.9494502544403076,\n",
       " 3.7933316230773926,\n",
       " 3.8412437438964844,\n",
       " 3.6062381267547607,\n",
       " 3.1827566623687744,\n",
       " 3.754180669784546,\n",
       " 3.5493228435516357,\n",
       " 3.6981797218322754,\n",
       " 3.844804286956787,\n",
       " 3.8680012226104736,\n",
       " 3.751868486404419,\n",
       " 3.7518298625946045,\n",
       " 4.086081027984619,\n",
       " 4.018080711364746,\n",
       " 3.027984619140625,\n",
       " 3.7981460094451904,\n",
       " 3.916383743286133,\n",
       " 3.5286242961883545,\n",
       " 3.971118688583374,\n",
       " 3.7457380294799805,\n",
       " 3.89684796333313,\n",
       " 3.9971606731414795,\n",
       " 3.6591761112213135,\n",
       " 3.7571661472320557,\n",
       " 3.6125428676605225,\n",
       " 3.8296420574188232,\n",
       " 3.8193883895874023,\n",
       " 2.9723827838897705,\n",
       " 3.695824146270752,\n",
       " 3.846801519393921,\n",
       " 2.792387008666992,\n",
       " 3.745819330215454,\n",
       " 2.881878614425659,\n",
       " 3.7991015911102295,\n",
       " 3.7809126377105713,\n",
       " 3.3484890460968018,\n",
       " 3.7972350120544434,\n",
       " 3.799952983856201,\n",
       " 3.8984813690185547,\n",
       " 3.8910884857177734,\n",
       " 3.9666128158569336,\n",
       " 3.7844951152801514,\n",
       " 3.5387842655181885,\n",
       " 3.6335978507995605,\n",
       " 4.026993274688721,\n",
       " 3.582838296890259,\n",
       " 4.0081963539123535,\n",
       " 3.535503625869751,\n",
       " 3.625319242477417,\n",
       " 3.4565956592559814,\n",
       " 3.725043535232544,\n",
       " 2.813655138015747,\n",
       " 3.6805577278137207,\n",
       " 4.036835193634033,\n",
       " 3.613443613052368,\n",
       " 3.8629939556121826,\n",
       " 3.922456741333008,\n",
       " 3.958378314971924,\n",
       " 3.8580682277679443,\n",
       " 3.851440191268921,\n",
       " 3.5044748783111572,\n",
       " 3.8201229572296143,\n",
       " 3.9720771312713623,\n",
       " 4.178720474243164,\n",
       " 3.6092092990875244,\n",
       " 3.929997205734253,\n",
       " 3.810121774673462,\n",
       " 3.713993787765503,\n",
       " 3.8607046604156494,\n",
       " 3.6186463832855225,\n",
       " 3.7918221950531006,\n",
       " 3.969364881515503,\n",
       " 3.750946283340454,\n",
       " 3.600358247756958,\n",
       " 3.818660020828247,\n",
       " 3.8527915477752686,\n",
       " 4.0753655433654785,\n",
       " 4.003436088562012,\n",
       " 3.684772253036499,\n",
       " 3.748980760574341,\n",
       " 3.7220523357391357,\n",
       " 3.857329845428467,\n",
       " 3.7199923992156982,\n",
       " 3.735585927963257,\n",
       " 3.8559930324554443,\n",
       " 3.57846999168396,\n",
       " 3.97879958152771,\n",
       " 3.7203073501586914,\n",
       " 3.7224395275115967,\n",
       " 3.4414641857147217,\n",
       " 3.651456594467163,\n",
       " 3.8667585849761963,\n",
       " 3.6349313259124756,\n",
       " 3.5004847049713135,\n",
       " 3.65747332572937,\n",
       " 3.9064011573791504,\n",
       " 3.856806516647339,\n",
       " 3.6610753536224365,\n",
       " 3.881004571914673,\n",
       " 3.816100835800171,\n",
       " 3.3932673931121826,\n",
       " 3.680056571960449,\n",
       " 4.057553768157959,\n",
       " 3.7667696475982666,\n",
       " 3.7747113704681396,\n",
       " 3.406574010848999,\n",
       " 3.839714288711548,\n",
       " 3.7712652683258057,\n",
       " 3.5628182888031006,\n",
       " 4.006248950958252,\n",
       " 2.8121185302734375,\n",
       " 3.5821878910064697,\n",
       " 3.4388601779937744,\n",
       " 3.74546217918396,\n",
       " 3.9968464374542236,\n",
       " 3.6689445972442627,\n",
       " 3.7025492191314697,\n",
       " 3.797887086868286,\n",
       " 3.9271388053894043,\n",
       " 3.827218770980835,\n",
       " 3.6704299449920654,\n",
       " 3.780991315841675,\n",
       " 3.5504560470581055,\n",
       " 3.7052769660949707,\n",
       " 3.636833429336548,\n",
       " 3.9001762866973877,\n",
       " 3.5300843715667725,\n",
       " 3.702868938446045,\n",
       " 3.812365770339966,\n",
       " 3.9351203441619873,\n",
       " 3.794227361679077,\n",
       " 3.5008292198181152,\n",
       " 3.6037652492523193,\n",
       " 3.9002373218536377,\n",
       " 3.4706499576568604,\n",
       " 3.9226841926574707,\n",
       " 3.8523948192596436,\n",
       " 3.8211138248443604,\n",
       " 3.8860905170440674,\n",
       " 3.002100706100464,\n",
       " 3.8335213661193848,\n",
       " 3.7473857402801514,\n",
       " 2.9504406452178955,\n",
       " 3.6252124309539795,\n",
       " 3.7923331260681152,\n",
       " 3.6182403564453125,\n",
       " 3.96181583404541,\n",
       " 3.930196523666382,\n",
       " 3.4929211139678955,\n",
       " 3.626662015914917,\n",
       " 3.445340871810913,\n",
       " 3.691234827041626,\n",
       " 3.8024322986602783,\n",
       " 3.7762086391448975,\n",
       " 3.7043263912200928,\n",
       " 3.434863328933716,\n",
       " 3.8911776542663574,\n",
       " 3.597764015197754,\n",
       " 3.8664798736572266,\n",
       " 3.7136070728302,\n",
       " 3.7075090408325195,\n",
       " 4.040042877197266,\n",
       " 2.835029363632202,\n",
       " 3.569504499435425,\n",
       " 3.8161861896514893,\n",
       " 3.893359422683716,\n",
       " 3.6594691276550293,\n",
       " 3.80607533454895,\n",
       " 3.994736909866333,\n",
       " 3.472402811050415,\n",
       " 3.6327836513519287,\n",
       " 3.9962449073791504,\n",
       " 3.5985054969787598,\n",
       " 3.7130563259124756,\n",
       " 2.7037429809570312,\n",
       " 3.4786245822906494,\n",
       " 3.8354971408843994,\n",
       " 3.6172893047332764,\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data in to a xlsx file as prescibed by Machine Hack\n",
    "Beer_Score = pd.DataFrame(predicted_values, columns=['Score'])\n",
    "writer = pd.ExcelWriter('Predicted Score.xlsx')\n",
    "Beer_Score['Score'].to_excel(writer, 'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
